{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Downloading Flask-2.3.2-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.9/96.9 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting blinker>=1.6.2\n",
      "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n",
      "Collecting itsdangerous>=2.1.2\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting click>=8.1.3\n",
      "  Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "     ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Collecting Werkzeug>=2.3.3\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "     ------------------------------------- 242.2/242.2 kB 14.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\박지현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\박지현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\박지현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.2)\n",
      "Installing collected packages: Werkzeug, itsdangerous, click, blinker, flask\n",
      "Successfully installed Werkzeug-2.3.7 blinker-1.6.2 click-8.1.6 flask-2.3.2 itsdangerous-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-win_amd64.whl (323 kB)\n",
      "     ------------------------------------- 323.1/323.1 kB 19.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\박지현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (2.30.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\박지현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\박지현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (2.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\박지현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\박지현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\박지현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp310-cp310-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 61.0/61.0 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 44.4/44.4 kB ? eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\박지현\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Installing collected packages: tqdm, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.0 multidict-6.0.4 openai-0.27.8 tqdm-4.66.1 yarl-1.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Using cached PyMySQL-1.1.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 6.46kB/s]\n",
      "c:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hyun9\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 642/642 [00:00<00:00, 310kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.58MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.67MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 863M/863M [07:34<00:00, 1.90MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 24.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "A compact submersible of Eliksni design finishes its descent through the hazy depths of Titan\\'s methane ocean. The craft\\'s seafloor landing kicks up a cloud of dark silt and microbial life shimmering like stars. The submersible\\'s dorsal airlock cracks open with a rush of bubbles, then slowly folds down into a ramp, allowing a trio of figures armored in deep diving gear to emerge. The submersible\\'s single floodlight sweeps across the ocean floor, revealing the alien landscape of twisting coral. \\n\\nFenchurch approaches one of the coral growths, running a gloved hand over its surface. \\\"These polyps…\\\" he mutters. \\\"Is this—\\\" He stops suddenly at the sound of a mechanical snap, and turns to see Chalco and Lisbon-13 plant a large, mechanical spire in the ground. Internal lights flicker on as the spire whirrs to life, creating a regulated field of water pressure around the submersible. \\n\\nFenchurch steps away from the coral, rubbing his fingers together. He looks to the spire as its sides open like a flower and release several drones, each outfitted with floodlights. The drones swim out ahead, revealing the disorienting flicker of what looks like the water\\'s surface but at an impossibly vertical angle. \\n\\n\\\"This way,\\\" Chalco directs as she turns to follow the drones. Fenchurch and Lisbon look at one another, steady themselves, and fall in line behind their fireteam leader. \\n\\n\\\"Stop me if you\\'ve heard this one before,\\\" Fenchurch says, anxiously checking the talisman clipped to his armor. \\\"Two Hunters and a Warlock walk into the deep…\\\"\n",
    "\n",
    "Chalco Yong disappears behind the shimmering curtain of surface tension. Lisbon-13 follows without hesitation, leaving Fenchurch to stare at the vertigo-inducing, vertical plane of water. He reaches out, hesitantly touching the surface, only for Lisbon to reach back, grab him by the wrist, and yank him through. \\n\\n\\\"Everis,\\\" Lisbon mutters once he pulls Fenchurch to the other side. The Warlock doesn\\'t even realize he\\'s hyperventilating. \\\"Get it together.\\\" He gives Fenchurch a warning look, then nods at Chalco walking ahead across the maddeningly dry ground.  \\n\\nIt takes Fenchurch a moment to realize that he is not underwater anymore. Liquid methane drips off his suit and turns to ice when it hits the compacted silt. The cavernous space is suffocatingly dark. It will be hours yet before the sun rises on this section of Titan, yet the lambent shades of starlight twinkling in the void beyond fill Fenchurch with a lurching sense of dread. \\n\\nHe wants to ask, \\'Where are we?,\\' but the words fail to get past his lips. Instead, Fenchurch gathers himself and joins Chalco at her side. \\n\\nChalco raises her hand, and three drones come to heel behind her. \\\"We are the only extant team of Hidden to visit this site.\\\" She sends the drones up into the air with a wave. \\\"What you are about to see is not to be discussed outside of official reports.\\\" \\n\\nFenchurch takes a step forward, squinting against the dark, until the drones sweep the terrain with their floodlights. Then, as the alien landscape comes into sharp focus, one word finally escapes his lips: \\n\\n\\\"Oryx.\\\"\n",
    "\n",
    "ACCESS: RESTRICTED \\nDECRYPTION KEY: LKV4PSM6ZQ$FEN-092 \\nREP #: 003-TAKEN-156 \\nAGENT(S): FEN-092 \\nSUBJ: TKO-300 Analysis Notes \\n\\nSpectrological analysis indicates concentration of Taken energy within the cellular structure of subject\\'s remains. Resonance indicative of active Darkness presence lacking both direction and willpower.  \\n\\nCellular analysis indicates slow but active mitosis, explaining the exaggerated size of the remains when compared to the subject\\'s last known physical dimensions. The mind is gone, but the body continues to grow. \\n\\nComparative analysis to samples with remains of subject WQS-030 do not match. WQS-030 shows no sign of mitosis. May be a compounding effect from worm severance and exposure to Light. \\n\\nAutopsy revealed no presence of worm carcass within TKO-300 remains. Status unknown. \\n\\nProlonged exposure to TKO-300 causes vivid audio-visual hallucinations due to combined exposure to Taken and Darkness energies. ERI-223 notes similarities to object of interest COS-030. \\n\\nCatalogued recollections from initial Guardian fireteam exposed to site indicate WGX-003 frequently visited the remains to grieve. TKO-300 is capable of absorbing knowledge and events but incapable of reacting to them.  \\n\\nIt is my professional recommendation that the remains of TKO-300 should be destroyed. It poses a threat to the safety of humanity far outweighing any potential knowledge value. \\n\\nMESSAGE ENDS \\n\\nACCESS: RESTRICTED \\nDECRYPTION KEY: MNXF8WTV9K$CHA-319 \\nREP #: 003-TAKEN-156 \\nAGENT(S): CHA-319 \\nSUBJ: FW: TKO-300 Analysis Notes \\n\\nAttached to FEN-092\\'s analysis are quarantine recommendations from ERI-223 and my own analysis of object NEV-000 for comparison to TKO-300. \\n\\nMy recommendation is as follows: reclassify POIs to segment queries, quarantine remains to off-site location, and limit direct exposure to frames and mechanical intelligence. There is too much of value here to simply throw away. \\n\\nMESSAGE ENDS\n",
    "\n",
    "\\\"Keep this talisman on you at all times.\\\" \\n\\nBlack, glossy, and in the right light, the talisman has a rainbow iridescence. It reminds Fenchurch of hematite pearls from the Bay of Drowned Wishes. He rolls it in his hand, looking up at Chalco to see the creases of worry in her brow. \\n\\nFenchurch clips the talisman to his jacket as the rest of his fireteam has. \\\"What\\'s it do?\\\" \\n\\n\\\"If it glows white, stay where you are and call out,\\\" Chalco explains, choosing her words carefully all while avoiding Fenchurch\\'s gaze. \\\"If you experience any memory loss or sense of déjà vu, call out.\\\" A silence falls over her fireteam. Palpable tension fills the cabin as they feel the craft begin to descend. \\n\\n\\\"If you see anyone at the operation site that you do not recognize, call out.\\\" Chalco continues, occasionally steadying herself with a hand on the cabin ceiling as the submersible pitches and yaws in the disruptive currents. \\\"If you experience any out-of-body sensations or missing time, call out.\\\" \\n\\n\\\"Ma\\'am?\\\" Lisbon-13 is not afraid to interrupt Chalco. His eyes glow bright from the back of the submersible. \\\"Where are we going?\\\" \\n\\nChalco stares for a moment, squares her shoulders, and continues. \\\"If you hear a voice that you think is your own…\\\"\n",
    "\n",
    "What appeared to be a silhouette of coral growths in the dark is revealed to be the mountainous remains of Oryx, the Taken King. The Hive god\\'s immense carcass lays strewn across Titan\\'s seafloor. Both Lisbon-13 and Fenchurch are taken aback by the sight, but Chalco advances toward it. \\n\\n\\\"The Lucent Brood performed a ritual here.\\\" Chalco motions to the shattered remnants of a Hive Ghost on the ground. \\\"We believe they were trying to forcibly resurrect Oryx by combining heretical Hive necromancy with the powers of that Ghost. They were disrupted.\\\" \\n\\nLisbon steps forward, unable to look away from Oryx\\'s cadaverous face. \\\"Is that possible?\\\" \\n\\n\\\"Officially: no. Unofficially…\\\" Chalco glances at Lisbon. \\\"That\\'s what we\\'re here to investigate. The body is still… active. With Taken energy and Darkness. The Guardians who intercepted this ritual reported finding free-floating fragments of consciousness on the way to this site. Memories from the corpse.\\\" \\n\\n\\\"It\\'s alive?\\\" Fenchurch whispers, hesitant to approach Oryx\\'s remains.  \\n\\nChalco shakes her head. \\\"That\\'s unclear. We need to bring back samples for further analysis. Ikora wants this entire site quarantined and the body exhumed, pending transport to a secure location for further analysis.\\\" \\n\\nFenchurch checks his talisman. Still black. He glances between Chalco and the corpse. \\\"Are we the first team to come down here?\\\" \\n\\nChalco looks at Fenchurch, then down to the ground. Her gaze lingers there for a moment in tense silence. Finally, she fixes her gaze ahead to the corpse. \\\"We are the only extant team of Hidden to have visited this site.\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I.\n"
     ]
    }
   ],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=4000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13921,  1637,  2822, 12157,    30, 50256, 26788, 24779, 12157,   837,\n",
       "           475,   340,   635, 24779,   257,  1256,   286,  1243,   326,   787,\n",
       "           345,  3772,   764, 50256, 50256, 26788, 24779, 12157,   837,   475,\n",
       "           340,   635, 24779,   257,  1256,   286,  1243,   326,   787,   345,\n",
       "          3772,   764, 50256, 50256, 26788, 24779, 12157,   837,   475,   340,\n",
       "           635, 24779,   257,  1256,   286,  1243,   326,   787,   345,  3772,\n",
       "           764, 50256, 50256, 26788, 24779, 12157,   837,   475,   340,   635,\n",
       "         24779,   257,  1256,   286,  1243,   326,   787,   345,  3772,   764,\n",
       "         50256, 50256, 26788, 24779, 12157,   837,   475,   340,   635, 24779,\n",
       "           257,  1256,   286,  1243,   326,   787,   345,  3772,   764, 50256]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m bot_input_ids \u001b[39m=\u001b[39m  new_user_input_ids\n\u001b[0;32m      8\u001b[0m     \u001b[39m# generated a response while limiting the total chat history to 1000 tokens, \u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m chat_history_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(bot_input_ids, max_length\u001b[39m=\u001b[39;49m\u001b[39m3000\u001b[39;49m, pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# pretty print last ouput tokens from bot\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDialoGPT: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(tokenizer\u001b[39m.\u001b[39mdecode(chat_history_ids[:, bot_input_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:][\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)))\n",
      "File \u001b[1;32mc:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\transformers\\generation\\utils.py:1538\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1532\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1533\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1534\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1535\u001b[0m         )\n\u001b[0;32m   1537\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1538\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[0;32m   1539\u001b[0m         input_ids,\n\u001b[0;32m   1540\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[0;32m   1541\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[0;32m   1542\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[0;32m   1543\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[0;32m   1544\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[0;32m   1545\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[0;32m   1546\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[0;32m   1547\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[0;32m   1548\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[0;32m   1549\u001b[0m     )\n\u001b[0;32m   1551\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[0;32m   1552\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\transformers\\generation\\utils.py:2362\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2359\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2361\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2362\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[0;32m   2363\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[0;32m   2364\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2365\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   2366\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   2367\u001b[0m )\n\u001b[0;32m   2369\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2370\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1076\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1077\u001b[0m     input_ids,\n\u001b[0;32m   1078\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1079\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1080\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1081\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1082\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1083\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1084\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1085\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1086\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1088\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1090\u001b[0m )\n\u001b[0;32m   1091\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1093\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:844\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    843\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwte(input_ids)\n\u001b[1;32m--> 844\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwpe(position_ids)\n\u001b[0;32m    845\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m position_embeds\n\u001b[0;32m    847\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\hyun9\\anaconda3\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "new_user_input_ids = tokenizer.encode(\"please summarize \" + text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "bot_input_ids =  new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "chat_history_ids = model.generate(bot_input_ids, max_length=4096, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
